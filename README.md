# Successive-Rank-Neural-Networks
What we have done here is effectively the opposite of Law-Rank Adapdation (LoRA). We train a feed-forward neural network, but with the weight matrices initially restricted to rank 1. Once the model has converged, we freeze the current weights and add a new trainable rank. The final weights will thus be sums of rank 1 matrices, similar to a Singular Value Decomposition (SVD). The analogous "singular vectors", are constrained to be orthogonal, as they are in an SVD. The parallels to the SVD do not end here--we find that the model learns the most important features first, w.r.t. the loss, and is refined with each rank update with diminishing returns. We find that models trained in this way avoid over-fitting where a regular neural network does. 
